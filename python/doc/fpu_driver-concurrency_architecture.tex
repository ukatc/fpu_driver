% -*- mode: latex; eval: (flyspell-mode 1) -*-
\documentclass[fontsize=12,a4paper]{scrartcl}


\usepackage[T1]{fontenc}

\usepackage[narrow]{inconsolata}
\usepackage{gentium}

\usepackage{gensymb}
%\usepackage{minted}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{framed}
%\usepackage{makeidx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage{rotating}

% \usepackage{tocloft}
% \setlength{\cftsecnumwidth}{2.5em}
% \setlength{\cftsubsecnumwidth}{3.5em}

\DeclareMathOperator{\round}{round}

\makeindex

\newenvironment{warning}{\begin{framed}\includegraphics[width=5em]{accident-area-ahead.png}
}{\end{framed}}


\pagestyle{headings}

\bibliographystyle{alpha}

\begin{document}

\title{Concurrency Architecture of the MOONS fpu\_driver Module}


\author{Johannes Nix}

\maketitle

\tableofcontents

\section{Related documents}

\begin{tabular}{|ll|}
  \hline
\verb+[1] VLT-TRE-MON-14620-1018+, &  FPU Verification Requirements \\
\verb+[2] VLT-TRE-MON-14620-3017+, & MOONS Fibre Positioner Verification Software Design \\
\verb+[3] VLT-TRE-MON-14620-3016+, & Fibre Positioner Communication Protocol\\
\verb+[4] FPUMovementSafetySpecification+ & Specification of safety levels for FPU movements\\
\verb+[5] VLT-ICD-MON-14620-4303+ & RFE to Instrument Software Interface Control Document \\
\hline
\end{tabular}


\section{Scope of this document}

\paragraph{Assumed knowledge} This technical description
assumes that the reader is familiar with multi-threaded programs and
especially the POSIX pthreads interface. It is assumed that the reader
is somewhat familiar with concepts like condition variables or atomic
operations.  A few references are provided for further reading.


\paragraph{Scope of this text} The goal of this document is to explain the main points of the
concurrency architecture of the \texttt{fpu\_driver}.  This is
necessary because, unlike many other aspects, in C/C++ source code,
the concurrency behaviour is not explicitly defined and visible.  One
would often need to scrutinize the entire source code to understand it
and check whether invariants are being kept.

At the same time, failure to ensure synchronized access in concurrent
code can be the cause for hard-to-debug undefined behaviour
\cite{Regehr:AGtUBPt3}\cite{MeyersAlexandrescu:2004:DCL}\cite[p
  218-225]{Love:2013:LSP}.

\section{Overview}

\subsection{Basic operation: Gateways, sockets and queues}

At the most fundamental level, the driver module generates CAN
messages which are sent to three different gateways, via three socket
connections\cite[chapters 56-61]{Kerrisk:TLPI}. The gateways forward
the messages to FPUs via 15 CAN buses, and the FPUs usually send
responses, which the gateways passes to the sockets. The responses are
received by the driver, which tracks the state of each FPU.

The driver needs to process up to 250,000 messages in a few seconds.
Therefore, it tries to send messages as fast as possible. The sockets
provide in principle a kind of networked FIFO: commands messages are
put in on one end, and read at the other end. If the associated
buffers are full, writing to a socket needs to wait until the reading
end has advanced its processing.

In practice, the bottleneck or limiting factor for speed is the speed
of the CAN buses, which are much slower than the TCP network
connections to the gateways. Because the bandwidth of the CAN buses,
is limited, the driver does its writing to the sockets in such a way
so that it does not lose waiting time: If it cannot write to one
socket, it looks whether there is a message which can be sent to
another socket, and if this is the case, it proceeds with this
message.

To do that, messages are stored in three queues, one for each
gateway. That means that when sending messages, it has to match
sockets which can transmit new messages with queues that have messages
that should be delivered.

The basic architecture is somewhat shaped like modern server
architectures which solve the so-called C10k problem, that is, being
able to maintain tens of thousands of connections in a single web
server process\cite{Kegel:C10kProblem}\cite{WP:C10kProblem}.

In traditional server architectures, which use one thread for every
connection, this is not possible, because the cost for switching
between threads is high, especially if many threads are
involved. Applied to the task of communicating with 1000 FPUs, it is
not efficient to have one thread for every FPU, or for every
gateway. Rather, there is exactly one thread which sends messages, and
one thread which receives them, and the method for sending and
receiving data are so-called non-blocking function calls.  This
reduces the amount of locking and synchronization needed.

\subsection{Concurrency architecture}
\begin{sidewaysfigure}[p]
\includegraphics[width=0.9\textwidth]{fpu-driver-concurrency-architecture.png}
\caption[Overview of Concurrency architecture of FPU driver]{Overview
  of Concurrency architecture of FPU driver. The green rectangles are
  threads, the blue rounded rectangles are shared data structures, and
  the arrows show how the threads access the data structures: broken
  blue arrows denote read-only access, and solid magenta arrows
  indicate modifying, or read-write access.}
\label{fig:overview}
\end{sidewaysfigure}

\subsection{Sockets}


Sockets are identified by socket file descriptors.  Using these socket
file descriptors is thread-safe, as long as not different threads try
to simultaneously write to, or read from, the same socket. The driver
architecture guarantees that this does not happen: Writing to the
sockets happens always in the tx thread, and reading from the sockets
happens always in the rx thread.  These operations are
thread-safe. Because sockets are normally not opened or closed during
the existence of the threads, their state does not need to be checked
apart from the check of return codes of the \texttt{send()} and
\texttt{recv()} operations. If a socket connection fails, the error is
detected by checking the return codes of \texttt{send()} and
\texttt{recv()}, the sending (tx) and receiving (rx) loops are exited,
and the threads are terminated.


\subsection{Threads}
Figure~\ref{fig:overview} gives an overview on the concurrency
structure in the \texttt{fpu\_driver} module.


The driver uses POSIX threads, which are explained in-depth in \cite[chapters 29 - 33]{Kerrisk:TLPI}
and \cite[chapter 7]{Love:2013:LSP}.
In the top row of the figure we see three rectangles,
which represent the three threads which are active
in the running \texttt{fpu\_driver} module. These are:

\begin{description}
\item[Control thread] The ``control''  thread is identical to the main thread of
  the program. It's functions are entered when new commands are sent
  to the FPUs, and return when the result of the commands is returned
  to the caller. In the time between, the thread could be
  blocked. This works almost exactly as in the same way
  as, for example, a \texttt{read()} or \texttt{write()} library
  call in a program that uses POSIX file IO.


\item[tx thread] The ``tx'' thread performs the sending of commands to
  the fibre positioners, by sending messages over three socket
  connections. Each socket is connected to one EtherCAN gateway. It is
  structured so that it can send messages as fast as possible. If the
  gateways or the FPUs cannot process messages as fast, it waits until
  one of the sockets is writable. The sockets are shared between tx
  thread and rx thread.

\item[rx thread] The ``rx'' thread reads any return messages from the
  fibre positioners to the driver, and updates the driver's internal
  structures which describe the state of the FPUs. It is designed so
  that it can read messages as fast as possible. Especially, the rx
  thread is designed so that it never has to wait itself for any
  long-running or potentially blocking operation, such as a file read,
  or a network response\footnote{There is one exception to this, which
    is logging of detailed debug messages if a high level of verbosity
    is selected. In this case, there are blocking calls to the log
    functions, which could block the rx thread temporarily. Such a
    high level will slow down operation and is not advisable for
    normal operation.}.

\end{description}


\subsection{Data structures}

The rounded boxes in the bottom of the figure represent the main data
structures which these three threads are accessing.

The arrows show the type of access. There are two types of access,
read-only access, which is symbolized by broken blue arrows, and
modifying access, which is depicted by magenta straight arrows. We see
that there are two data structures which are different, in that they
only receive read-only access from all three threads. As matters
around them are simpler, we will discuss them first.

\subsubsection{Read-only data}

The read-only data structures are as follows:

\begin{description}
\item[Driver configuration] This structure holds the parameters which
  define the global configuration of the driver, for example the
  minimum motor frequency required in waveforms. It has the type
  \texttt{EtherCANInterfaceConfig}, and is passed as a constant
  argument to the driver's constructor. Because it is in no place
  modified, we know that it does not change, and we can pass it
  everywhere as constant (similar as a string in Python)\footnote{In
    the core FPU driver, these parameters are passed as a constant
    structure. Passing the configuration in memory has the advantage
    that the core driver does not become dependent on the ESO VLT
    framework, and this makes it possible to test it within the FPU
    verification system, which cannot depend on that framework.}

\item[FPU address configuration] This structure is a table which maps
  the logical FPU numbers to the address triplet of (gateway number,
  bus number, and CAN ID on the bus). The table is created at program
  start-up, and passed to the threads when they are created\footnote{
    As is the case with the driver parameters, the address
    configuration is determined at start-up time. Currently, the
    address table is passed to the method
    \texttt{GatewayInterface::connect()}. The final driver version
    will read the addresses from a configuration file and, for
    example, pass it to the \texttt{connect()} method.}.

  Synchronization of the table poses an interesting question: If the
  table is created at program start-up, it is modified in the control
  thread. After this, the tx thread and the rx thread are created. How
  can we be sure that this modification that happens at start-up is
  not visible in the newly created threads, making the structures
  non-constant and our assumptions invalid?

  The answer to this is that the creation of threads in pthreads
  causes a so-called memory barrier
  \cite{WP:MemoryBarrier}\cite{Preshing:2012:MemoryBarriers}\cite[section
    7.1]{Butenhof:PwPT}.  Any data which was written by the CPU before
  that thread was created, is written to memory before the new thread
  can see it. This is important, because normally in a C program with
  different threads, there is by default \emph{no} notion of
  ``before'' or ``after'' for operations in different threads, unless
  such operations are explicitly synchronized. The thread creation
  represents such an synchronization point.



\end{description}

\subsection{Message queues}
Furthermore, we have three main types of data structures which are not
only read concurrently, but also modified. We discuss the
\emph{message queues} first.

Message queues are thread-safe data structures which allow the control
thread to send parametric messages for individual FPUs to the tx
thread. The control thread is responsible for setting up the messages
and its actual data content, and the tx thread is responsible for
sending them to a gateway.

To achieve that, the control thread inserts each message into a data
structure which is basically a thread-safe FIFO or ring buffer.  The
insertion method uses locks to ensure that the internal invariants of
the data structure are kept correct, and that no unsynchronized
concurrent modifications occur. There is one message queue for each
socket, so that we have three of them. Using the FPU's address
configuration, the control thread knows into which queue to insert a
message.

The tx thread checks whether there are messages in the message queues,
\emph{and} whether it can send a message to a socket without blocking
(this is done using the \texttt{ppoll()} syscall
\cite[p. 51]{Love:2013:LSP})). If this is the case, the message is
serialized into a byte sequence and written to the socket; the
remaining processing is handled by the OS kernel.



\subsection{Message pools}

One aspect which needs to be discussed now is what happens with the
message after it was processed by the rx thread. This is related to
how messages are represented as objects in memory. Messages could be
just values which are copied, and handled by a few functions. However,
it is actually useful to treat them as object instances which have
virtual methods, supporting some degree of abstraction by using
object-oriented programming techniques.

As such objects can have different sizes, they need to be passed by
pointers, and their memory needs to be managed.  This is done using
C++ smart pointers (more precisely, \texttt{std::shared\_ptr<>}, which
are supported by C++11 and newer versions of C++). In theory, each
message object could be allocated, using \texttt{new}, when it is
needed, and deallocated, using \texttt{delete}, when it is not longer
needed. However this is relatively slow. Therefore, when messages
instances are needed, these are taken from a set of message pools,
which are created on program start-up, and these instances are
inserted back into the corresponding pool when they are not needed any
more at the moment.

The resulting pattern is that the control thread takes a message
instance from a message pool, and inserts a reference to it into a
message queue. Then, the tx thread pops it from the message queue,
generates and sends the message's byte representation to the socket it
is addressed to, and recycles the message memory by adding its
reference again to the corresponding message pool. With this scheme,
no dynamic allocation of messages is needed after program start-up, so
this is much faster.

As a detail, there is one pool for each type of CAN message, and the
pools are simpler organized than the message queues: They are
basically a stack of smart pointers, implemented as a
\texttt{std::vector<std::shared\_ptr<\ldots>>>}.

\subsection{FPU state handling}

After looking at the message queues and message queues, we can now
look at the remaining data structure, which is more complex, and very
central to the FPU driver software. It is represented by the larger
rectangular box in the middle of Figure~\ref{fig:overview}, and
provides an array of structs which record the FPU's state with each
response message that was received from the gateway. Because we have
one such state record for each of the 1001 FPUs, this structure is
called the \texttt{FPU\_state} array, or \texttt{FPUArray}.

For the moment, we will temporarily ignore the box which is labelled
``\texttt{TimeOutList}'' (it will be explained soon).  The response
messages are received by the rx thread, of course.  Upon receipt, the
rx thread writes the state updates into the FPU state array,
symbolized by the magenta arrow. The control thread reads from the
state array. To avoid corruption by concurrent modification, both
threads use a lock or Mutex, which ensures that write accesses and
read accesses are mutually exclusive. The rx thread keeps this lock
most of the time. Only for short moments, the lock gets released,
which gives the control thread the opportunity to read back the new
state data, and return it to the caller of the driver method.

\paragraph{Transfer of state data from rx to control thread}
As already mentioned, the FPU state structure is updated by the rx
thread, but it is also passed to the control thread, and its
information is required by the higher levels of application software
which use the driver methods. In theory, this data could be protected
by holding more locks. But this would violate the concern that we want
the rx thread to accept and store that data as fast as possible.

Instead, the FPU state data is \emph{copied} when it is passed from
the rx thread to the control thread. Only while that copy is
performed, it needs to be locked for the rx thread. And because the
copy is a fast memory operation (the \texttt{memcpy() function call})
with nothing that can block, the time which the lock needs to be hold
by the control thread is very short. As a result, the control thread
methods return a copy of the FPU state data, which can be processed
conveniently without any locking. The exists an API method which
retrieves the current state, which is called \texttt{getGridState()}.

\subsection{The \texttt{TimeOutList} data structure}

Noe, let's have a look at the data structure \texttt{TimeOutList},
which is written to by both the tx thread, and the rx thread.  

Its function is as follows:

Normally, most messages to an FPU are promptly followed by a response
within a few milliseconds. However, it can happen that this runs into
errors, for example if the connection to the CAN bus gets broken. In
this case, the response will never arrive, and the driver needs to
handle this.

This is done by maintaining a sorted list of time-out values, each
representing an absolute time. The list works like sorted set or a
heap (with some efficiency improvements for this specific case). Each
time when a message is sent to an FPU, an expiration time for the
message is computed using the message's class interface\footnote{The
  time-out values are specific to each type of CAN command, and are
  defined by constants in the headers defining the specific command
  class, in the folder \texttt{include/ethercan/cancommandsv2}.}, and
this time is added as a time stamp to the list of time outs.

Each time when the rx threads starts to wait for a new message, it
looks for the next time-out, and waits at most until this moment.  In
the normal case, some FPU returns a message before this time expires,
and the response is processed normally in the rx thread, by updating
the corresponding FPU's state structure in memory. When doing this,
the pending time-out entry is simply cleared.

If, however, the waiting time ends without a response, all FPUs whose
time-out time stamp have expired, are marked as timed out, and the
entry is removed from the time-out list.  This structure ensures that
all FPU responses can be processed ``in parallel'', and only the error
case that a response is missing requires a longer waiting time.

To manage time-outs in such a way, the \texttt{TimeOutList} data
structure needs to be modified by both the tx thread and the rx
thread, which is why it is a special component of the FPU state
array. Because it can be modified directly, the \texttt{TimeOutList}
has an own lock, which can be acquired without holding the mutex for
the FPU state array.

As a detail, time values and clocks are based on the monotonic system
clock (time-keeping in the Linux kernel is explained in more detail in
\cite[chapter 11]{Love:2013:LSP}). This ensures that there are no
``jumps'' in the used time values when detecting time-outs.

\section{Relationship between threads and sockets}

To understand error handling in the concurrent parts of the driver, it
is important to know the relationship between threads and sockets.
In short, \textbf{threads and open sockets depend mutually on each other:}

\begin{itemize}
\item 
  When a socket is opened, it needs a thread to process
  any messages from the FPUs. If there is no thread
  which reads and processes the FPU reply messages,
  the gateway would need to discard messages,
  or the CAN bus would become blocked. The buffer space
  for messages from the FPUs to the driver on the
  EtherCAN gateway is very limited. Therefore, if there
  is a socket, one needs also a thread which handles
  messages on it.
  
\item Conversely, threads cannot do any work without sockets from
  which they can read, or to which they can send messages.  If a
  socket becomes closed, this is with certainty caused by a fatal
  error which cannot be handled at the level of the thread.  Also, if
  sockets are open to some gateways, and not open to other gateways,
  this causes inconsistencies, for example when a movement is started
  using the \texttt{executeMotion()} command -- some FPUs will be
  reached by the command (and will move), and other not (and will
  likely collide with the moving ones). To sum up, threads need
  sockets to work.

  
\end{itemize}

The solution to this is that threads and sockets are kept bundled as a
package - as soon as the sockets are created, also the corresponding
threads are started, and when one thread is stopped or when one socket
becomes closed, all threads are stopped, and all sockets are closed.

The thread and socket creation happens in the FPU driver's
\texttt{connect()} method, and their termination happens in the
\texttt{disconnect()} method (which is also called by the destructor).




\section{Conditional waiting and event messaging}

As outlined above, the methods executed in the control thread (for
example, \texttt{configMotion()}) work similar to standard system
calls like \texttt{read()} or \texttt{write()}, in that they block
and wait until a driver operation is completed, and return with the
result.

In the case of the \texttt{fpu\_driver}, this means that fairly complex
operations have to complete, because messages are sent to a large set
of 1001 FPUs, and normally we want to wait until each of them has
responded that the command is complete.

Now, this could be implemented in a way that the rx thread holds a
lock on the array for the state of all FPUs, then when a message
arrives, it updates the states, releases the lock, then the control
thread gets the lock, checks all FPU state records whether the
operation is complete, if not it releases the lock, and so on. This
has several disadvantages:

\begin{itemize}
\item It is over-involved in terms of abstraction, because the control
  thread is not really interested in the result for each of the 1000
  FPUs individually, but rather in the end result of the operation.
\item It is slow, because in the time in which the control thread holds
  the lock, the rx thread cannot do its work and update the
  state with new FPU responses.
\item Its is furthermore quite inefficient, because each time the CPUs
  switch threads, the whole FPU state data structure needs to be
  synchronized between CPU caches, and the OS needs to perform a
  context switch to activate the other thread.
\end{itemize}

\subsection{Condition variables}

To improve on this, pthreads \textbf{condition variables} are used \cite[section 3.3, p70]{Butenhof:PwPT} \cite[chapter 4]{Williams:CppConcurrency}.
Basically, they allow to suspend the control thread until a certain
condition has been reached. This condition is checked by the rx
thread. This means that the CPU does not need need to switch
constantly between the two threads. Rather, the rx thread has some
description of the specific condition that the control thread waits
for, and only if that condition is probably reached, the control
thread is woken up and can check the state of things.

Condition variables are used in two places in the driver:

\begin{enumerate}
\item In the function \texttt{GatewayInterface::waitForState()} which
  waits for a bulk command operation to the
  FPU grid to complete. This function is executed
  in the control context, and checks whether the FPU
  states which are recorded by the rx thread have
  reached the desired final state (or an error).
\item In the function \texttt{CommandQueue::waitForCommand()}, a
  conditional wait is performed if no new messages are present in the
  message queue. The wait finishes if a new message is added to one of
  the queues, or the associated time-out expires.
  
\end{enumerate}


\subsection{Counters of pending messages}

As mentioned, the method \texttt{waitForState()} waits for specific
commands to the FPU grid to complete.  It would be possible to check
the state of the whole FPU grid every time after which an FPU has
received a response message. But this would be inefficient, as the FPU
state array is relatively large, and the check would needs to scan its
whole memory each time, from a different thread. This would be costly,
because CPU caches need to be synchronized.

Instead, two counters of pending messages are maintained, which
correspond to all FPUs together:

\begin{itemize}
\item One counts which FPUs have messages being sent to, without that
  a response has arrived. This counter works by that every time a
  message is send which needs a response, the counter is incremented,
  and every time a response arrives, it is decremented. Every time
  which a time-out for an FPU occurs, the counter is decremented as
  well.
\item The second counter is incremented each time which a message,
  or a set of messages, is added to one of the message queues,
  and decremented each time after one of these
  message requests was processed.
\end{itemize}

The temporal order in which both counters are changed is important:
The count of messages pending in a message queue can only be
decremented after the count of pending messages to the FPUs has be
incremented. Otherwise, the driver could assume that all messages have
been sent, in an instant where are no messages in the message queue,
but some of these messages have not yet been sent to the FPUs.
Counters are implemented as atomic data types \cite[see chapter
  5]{Williams:CppConcurrency}. Counters which are used in conditional
waits still need to be accessed using a mutex when they are modified,
because checking condition variables requires to hold an associated
mutex.



\subsection{State flags}

There are two state flags which are shared between the rx thread and
the tx thread, and are used to signal termination of the threads. Like
the counters, these are implemented as atomic data types \cite[see chapter 5]{Williams:CppConcurrency}.

\subsection{Event file descriptors for interrupting ppoll()}

The two message loops for outgoing and incoming messages both use the
\texttt{ppoll()} syscall to check whether a socket becomes writable
(for sending a new message) or readable (for reading a new response
message) \cite[p. 51, p61]{Love:2013:LSP}. If the sockets do not allow
such an operation, both threads wait for a limited time (0.5 seconds
for the rx thread, and 10 seconds for the tx thread). However there a
few situations where this wait should be interrupted sooner:

\begin{itemize}
\item If the tx thread is waiting for a socket to become
  writable, and a new message arrives for another socket,
  which might be writable.
\item When the driver is being shut down.
\item If a fatal error occurs on any of the reading or writing socket
  and the sockets should be shut down.
\end{itemize}

For these cases, Linux \emph{event file descriptors} are used, which
are special file descriptors that are created by the
\texttt{eventfd()} syscall \cite{man:eventfd}.  This are basically
file-like objects that can message events, which then can be detected
in another part of a process using the \texttt{ppoll()} call.  Because
\texttt{ppoll()} can wait for any number of file descriptors, it can
wait for alternative events at the same time.

For the case of new messages for the tx thread, there are two specific
situations in which the above is helpful:

\begin{itemize}
\item If a normal new message was created for a socket which is
  currently writable, it increases throughput if the rx thread does
  not wait for the normal termination of the \texttt{ppoll()} call.
  
\item An important special case is if an \texttt{abortMotion()}
  message is sent. In this case, all previous pending messages are
  cleared from the message queue, and a message is sent to all
  sockets. In this case, it would be incorrect to wait even a short
  extra time for sockets to become writable - the CAN message
  corresponding to \texttt{abortMotion()} should be sent as fast as
  possible to every socket which is writable.
  
\end{itemize}

Here, an event which creates a new message for the tx thread, or
causes termination of the thread loops, is communicated by writing to
such event file descriptors. These descriptors are created right
before creating the threads. It is worth noting that, unlike condition
variables, these file descriptors need to be reset to their original
state by reading from them, before they can be used again.

\section{A selection of relevant classes and their synchronization objects}

This section gives an overview on classes managing concurrency, and
the synchronization objects used.

\begin{description}
\item[EtherCANInterface] This class uses the pthreads mutex
  \texttt{command\_creation\_mutex} to ensure that only one member
  function can send commands to the FPUs at the same time.
\item[GatewayInterface] This class manages access to the EtherCAN
  gateway and manages the threads.  It has the class members
  \texttt{fpuArray}, \texttt{timeOutList}, \texttt{commandQueue}, and
  \texttt{command\_pool}, each of which manage mutexes to operate in a
  thread-safe way. It uses two atomic flags (using
  \texttt{std::atomic<bool>}), \texttt{exit\_threads} and
  \texttt{shutdown\_in\_progress}, to initiate termination of threads.
  Furthermore, it uses two Linux event file descriptors,
  \texttt{DescriptorCommandEvent} and \texttt{DescriptorCloseEvent} to
  signal arrival of new commands, and closing of the sockets.

\item[FPUArray] uses the mutex \texttt{grid\_state\_mutex} and the
  condition variable \texttt{cond\_state\_change} to manage the FPU
  state array. It also uses an atomic counter
  \texttt{num\_trace\_clients} which modifies the behaviour of the
  \texttt{waitForState()} method to return after every single change
  of state\footnote{The \texttt{num\_trace\_clients} flag is only used
    for debugging purposes, it allows to shorten the wait time and
    inspect each change state individually.}.
\item[TimeOutList] uses the mutex \texttt{list\_mutex} to manage
  accesses to the list of time-outs, and retrieve the next pending
  time-out value.
\item[CommandQueue] uses the mutex \texttt{queue\_mutex} along with
  the condition variable \texttt{cond\_queue\_append} to manage the
  queue, and signal when a new command is appended to an empty queue.
  It also uses an event file descriptor
  \texttt{EventDescriptorNewCommand}, which is owned and created by
  the \texttt{GatewayInterface} class, and passed via a setter method.
\item[CommandPool] uses the mutex \texttt{pool\_mutex} and the
  condition variable \texttt{cond\_pool\_add} to manage changes to the
  command memory pool, and the event of adding a message to an empty
  pool\footnote{Handling an empty pool is a robustness feature, the
    command pool should contain so many instances when starting up
    that it never runs empty during driver operations}.
\end{description}

\appendix

\section{Class methods relevant for concurrency}

\subsection{EtherCANInterface}

This class provides the API which is accessible for
users of the library. The main actual function it
performs is that it makes sure that only one
command is active at the same time (with the exception of
the \texttt{abortMotion()} command, which has the special
property that it can preempt other commands).

\begin{description}
\item[EtherCANInterface()] (the constructor) creates member instances
  and synchronization objects
\item[all command methods] acquire the mutex \texttt{command\_creation\_mutex}
  while sending a new command
\item[abortMotion()] is a special case as the above lock is only acquired \emph{after} sending the
  abort messages.
  \item[waitForState()] waits for a set of CAN messages to complete
\end{description}

\subsection{AsyncInterface}

This class implements generating, configuring, and
sending CAN messages to the tx thread, and waiting
for a response from the rx thread.

\begin{description}
  \item[connect()] creates the connections to the gateways, by calling \texttt{GatewayInterface::connect()}
  \item[disconnect()] closes the connections
\end{description}


\subsection{GatewayInterface}
This class implements creating the sockets, event file descriptors,
condition variables and locks, and both the tx and the rx thread.

It also manages the registration and handling of time-outs.

\begin{description}
  \item[connect()] does the actual socket and thread initialization
  \item[disconnect()] stops the threads, and closes the sockets
  \item[waitForState()] implements waiting for a specific FPU grid state
  \item[getGridState()] is a method which copies the FPU array
    state while holding the associated lock.
\end{description}

\subsection{CommandQueue}
This data structure provides a set of thread-safe FIFOs which contain
messages that were created by the control thread, and need to be send
by the tx thread.  All operations are protected by a mutex.

\begin{description}
\item[enqueue()] adds a new command to the queue. It also
  signals the condition variable
    \texttt{cond\_queue\_append} and the event file descriptor \texttt{EventDescriptorNewCommand}.
  \item[checkForCommand()] checks whether there is a new command
  \item[waitForCommand()] waits a limited time for a new command,
    if the queue is empty. This wait uses a condition variable
    \texttt{cond\_queue\_append}, which is signalled when
    a new command is added to an empty queue.
  \item[flushToPool()] flushes all existing commands to the
    message pool. This is needed before an \texttt{abortMotion()}
    command is sent.
  \item[setEventDescriptor()] takes the event file descriptor for the
    event ``new command added to an empty queue'', and registers it
    for internal use as the member
    \texttt{EventDescriptorNewCommand}. This use of a resource that is
    not owned is needed because the file descriptor is shared with
    other objects, and is created by
    \texttt{GatewayInterface::connect()}.

\end{description}

\subsection{CommandPool}
The command pool class template implements a thread-safe container
that is protected by a mutex while it is changed. The mutex is called
\texttt{pool\_mutex}. The template parameter is the element type which
is managed. Each CAN command class has its own pool. Messages are
usually added in the tx thread, and retrieved in the control thread
(this can be different if an \texttt{abortMotion()} command is
executed).

\begin{description}
  \item[provideInstance()] retrieves an available CAN message object instance.
  \item[recycleInstance()] takes an instance which is not longer
    needed, and adds it to the pool. Adding an instance to an empty
    pool also signals the condition variable \texttt{cond\_pool\_add}.
\end{description}

\subsection{FPUArray}

This structure holds the state of the FPUs, and the driver.  It is
accessed by all three threads. It uses the mutex
\texttt{grid\_state\_mutex} to synchronize changes and data
retrievals, and the condition variable \texttt{cond\_state\_change} to
wait for specific states to be reached.

Additionally, it uses the atomic counter \texttt{num\_trace\_clients}
which determines whether the condition variable should only be
signalled when a whole set of FPUs has reached a new state, or
whenever a single FPU's state was changed (tracing mode).  The latter
can be used, for example, to retrieve data for a graphical real-time
display which shows the FPU's positions.

\begin{description}
  \item[getGridState()] return a copy of the data describing the state of the FPUs.
  \item[getInterfaceState()] return the state of the driver
  \item[getStateSummary()] return summarized data on the state of the FPU grid
  \item[waitForState()] waits until a specific target state, or an error condition,
    has been reached, and return with the new state.
  \item[setPendingCommand()] Set a command ID and a time-out value for an FPU
    as the current command for which a response is pending.
  \item[incSending()] increase counter of commands being sent.
  \item[decSending()] decrease counter of commands being sent.
  \item[countSending()] get number of commands which are being sent.
\end{description}

\subsection{TimeOutList}

This data structure provides a thread-safe sorted list of
time-outs. It is functionally similar to a min heap, and allows to add
time-outs, to retrieve the smallest value, and to update the time-out
for an FPU. All operations on it are protected by a lock, with the
name \texttt{list\_mutex}.

\begin{description}
  \item[insertTimeOut()] insert a time-out value for a specific FPU id.
  \item[clearTineOut()] clears the time-out value for a specific FPU.
  \item[getNextTimeOut()] get the time of the next time-out event,
    and the FPU id associated with it, without removing from the queue.
  \item[pop()] find and remove the time-out and FPU id with the
    minimum value.
\end{description}

\newpage
\bibliography{concurrency}



\end{document}
